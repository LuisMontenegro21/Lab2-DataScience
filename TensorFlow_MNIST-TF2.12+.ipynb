{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Red Neuronal Profunda (DNN) para clasificación MNIST\n",
    "\n",
    "Aplicaremos todos nuestros conocimientos para crear una DNN, frecuentemente llamada también una Artificial Neural Network (ANN).  El problema que vamos a trabajar se conoce como el \"Hola Mundo\" del aprendizaje profundo porque para la mayoría de estudiantes este es el primer algoritmo de aprendizaje profundo que ven. \n",
    "\n",
    "El conjunto de datos se llama MNIST y se refiere al reconocimiento de dígitos escritos a mano.  Pueden encontrar más información en el sitio web de Yann LeCun (Director of AI Research, Facebook).  El es uno de los pioneros de todo este tema, así como de otras metodologías más complejas como las Redes Neurales Convolucionales (CNN) que se utilizan hoy día.\n",
    "\n",
    "El conjunto de datos tiene 70,000 imágenes (28x28 pixels) de dígitos escritos a mano (1 dígito por imagen).\n",
    "\n",
    "La meta es escribir un algoritmo que detecta qué dígito ha sido escrito.  Como solo hay 10 dígitos (0 al 9), este es un problema de clasificación con 10 clases.\n",
    "\n",
    "Nuestra meta será construir una RN con 2 capas escondidas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan de Acción para preparar el modelo\n",
    "\n",
    "1.  Preparar los datos y preprocesarlos.  Crear los conjuntos de datos para entrenar, validar y probar\n",
    "2.  Crear un esboso del modelo y seleccionar las funciones de activación\n",
    "3.  Fijar los optimizadores avanzados y la función de pérdida\n",
    "4.  Hacer que el modelo aprenda\n",
    "5.  Probar la exactitud (\"accuracy\") del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar los paquetes relevantes\n",
    "\n",
    "TensorFlow incluye un proveedor de los datos de MNIST que utilizaremos acá.  Viene con el módulo **\"tensorflow.keras.datasets\"**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente instrucción, cuando se corre por primera vez, descarga el conjunto de datos en lo indicado por el parámetro path, relativo a  ~/.keras/datasets).  Como si se hubiera ejecutado Lo siguiente:\n",
    "\n",
    "tf.keras.datasets.mnist.load_data(\n",
    "    path = 'mnist.npz'\n",
    ")\n",
    "\n",
    "luego separa los datos en un conjunto para entrenamiento y otro para pruebas.\n",
    "\n",
    "Si se ejecuta más de una vez, ya no descarga el archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "(X_entreno, y_entreno), (X_prueba, y_prueba) = tf.keras.datasets.mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_entreno"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_entreno.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como no podemos ver la forma de los conjuntos...les queda de tarea averiguar por qué no...podemos utilizar la instrucción **assert**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert X_entreno.shape == (60000, 28, 28)\n",
    "assert X_prueba.shape == (10000, 28, 28)\n",
    "assert y_entreno.shape == (60000,)\n",
    "assert y_prueba.shape == (10000,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos\n",
    "\n",
    "Esta sección es donde pre-procesaremos nuestros datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por default, TF2 tiene conjuntos de datos de entrenamiento y de prueba, pero no tiene un conjunto de validación, por lo que debemos dividirlo por nuestra cuenta\n",
    "\n",
    "Lo haremos del mismo tamaño que el conjunto de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_obs_validacion = y_prueba.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos una variable dedicada para el número de observaciones de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_obs_prueba = y_prueba.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalmente preferimos \"normalizar\" nuestros datos en alguna forma para que el resultado sea numéricamente más estable.  En este caso simplemente preferimos tener entradas entre 0 y 1, por lo que definimos una función, que reciba la imagen MNIST.\n",
    "\n",
    "Como los posibles valores de las entradas son entre 0 y 255 (256 posibles tonos de gris), al dividirlos por 255 obtenemos el resultado deseado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_entreno_normalizado = X_entreno / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, normalizaremos y convertiremos los datos de pruebas en tandas.  Los normalizamos para que tengan la misma magnitud que los datos de entrenamiento y validación.\n",
    "\n",
    "No hay necesidad de \"barajearlo\" ya que no estaremos entrenando con los datos de prueba.  Habra una sola tanda, igual al tamaño de los datos de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_prueba_normalizado = X_prueba / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez se han \"normalizado\" los datos, podemos proceder a extraer los datos de entrenamiento y de validación.\n",
    "\n",
    "Nuestros datos de validación serán 10000 para ser igual al conjunto de prueba.\n",
    "\n",
    "Finalmente, creamos una tanda con un tamaño de tanda igual al total de muestras de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_validacion = X_entreno_normalizado[-num_obs_validacion: , : , : ]\n",
    "y_validacion = y_entreno[-num_obs_validacion:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarmente, los datos de entrenamiento son todos los demás por lo que nos salteamos tantas observaciones como las hay en el conjunto de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_entreno = X_entreno_normalizado[ : X_entreno_normalizado.shape[0] - num_obs_validacion, : , : ]\n",
    "y_entreno = y_entreno[ : y_entreno.shape[0] - num_obs_validacion]\n",
    "num_obs_entreno = y_entreno.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertir de Arreglos Numpy a Tensores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_entreno = tf.data.Dataset.from_tensor_slices((X_entreno, y_entreno))\n",
    "datos_validacion = tf.data.Dataset.from_tensor_slices((X_validacion, y_validacion))\n",
    "datos_prueba = tf.data.Dataset.from_tensor_slices((X_prueba, y_prueba))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Barajear y hacer tandas con el conjunto de datos de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAMANIO_TANDA = 100\n",
    "datos_entreno = datos_entreno.shuffle(buffer_size = num_obs_entreno).batch(TAMANIO_TANDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacer tandas con los conjuntos de validación y prueba, no se necesita barajearlos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_validacion = datos_validacion.batch(TAMANIO_TANDA)\n",
    "datos_prueba = datos_prueba.batch(TAMANIO_TANDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delineamos el modelo\n",
    "\n",
    "Cuando pensamos sobre un algoritmo de aprendizaje profundo, casi siempre imaginamos la realización del mismo.  Asi que esta vez, hagámoslo.  :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tamanio_entrada = 784\n",
    "tamanio_salida = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos el mismo ancho para ambas capas escondidas.  (No es una necesidad!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tamanio_capa_escondida = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definimos cómo se verá el modelo\n",
    "\n",
    "La primera capa (la de entrada):  cada observación es de 28x28 píxeles, por lo tanto es un tensor de rango 2.\n",
    "\n",
    "Como aún no hemos aprendido sobre CNNs, no sabemos como alimentar este tipo de entrada a nuestra red, por lo tanto hay que \"aplanar\" las imágenes.  Hay un método conveniente **Flatten** que toma nuestro tensor de 28x28 y lo convierte en  un vector (None,) o (784,)...porque 28x28 = 784.  Esto nos permite crear una red de alimentación hacia adelante.\n",
    "\n",
    "    \n",
    "**tf.keras.layers.Dense** básicamente implementa:  *salida = activation(dot(entrada, peso) + sesgo)*.  Requiere varios argumentos, pero los más importantes para nosotros son el ancho de la capa escondida y la función de activación.\n",
    "\n",
    "La capa final no es diferente, solo nos aseguramos de activarla con **softmax**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lpmon\\Documents\\GitHub\\Lab2-DataScience\\venv\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "modelo = tf.keras.Sequential([\n",
    "\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)), # capa entrada\n",
    "    \n",
    "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='relu'), # 1era capa escondida\n",
    "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='relu'), # 2nda capa escondida\n",
    "\n",
    "    tf.keras.layers.Dense(tamanio_salida, activation='softmax') # capa salida\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seleccionar el optimizador y la función de pérdida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Definimos el optimizador que nos gustaría utilizar, la función de pérdida, y las métricas que nos interesa obtener en cada interacción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento\n",
    "\n",
    "Acá es donde entrenamos el modelo que hemos construído\n",
    "\n",
    "Determinamos el número máximo de épocas.\n",
    "\n",
    "Ajustamos el modelo , especificando:\n",
    "\n",
    "* los datos de entrenamiento\n",
    "* el número total de épocas\n",
    "* y los datos de validación que creamos en el formato (entradas, metas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "500/500 - 1s - 2ms/step - accuracy: 0.8760 - loss: 0.4346 - val_accuracy: 0.9426 - val_loss: 0.2138\n",
      "Epoch 2/5\n",
      "500/500 - 1s - 1ms/step - accuracy: 0.9410 - loss: 0.2003 - val_accuracy: 0.9556 - val_loss: 0.1614\n",
      "Epoch 3/5\n",
      "500/500 - 0s - 981us/step - accuracy: 0.9544 - loss: 0.1527 - val_accuracy: 0.9613 - val_loss: 0.1363\n",
      "Epoch 4/5\n",
      "500/500 - 0s - 964us/step - accuracy: 0.9623 - loss: 0.1252 - val_accuracy: 0.9607 - val_loss: 0.1352\n",
      "Epoch 5/5\n",
      "500/500 - 0s - 990us/step - accuracy: 0.9689 - loss: 0.1048 - val_accuracy: 0.9670 - val_loss: 0.1143\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x15d9df1b200>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUMERO_EPOCAS = 5\n",
    "\n",
    "modelo.fit(datos_entreno,\n",
    "          epochs = NUMERO_EPOCAS, \n",
    "          validation_data = datos_validacion,\n",
    "          verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probar el modelo\n",
    "\n",
    "Como se discutió en clase, luego del entrenamiento (con los datos de entrenamiento), y la validación (con los datos de validación), probamos el potencial de predicción final de nuestro modelo con el conjunto de datos de prueba que el algoritmo NUNCA ha visto antes.\n",
    "\n",
    "Es muy importante reconocer que estar \"jugando\" con los hiperparámetros sobre-ajusta el conjunto de datos de validación.\n",
    "\n",
    "La prueba es la instancia absolutamente final. **NUNCA** debe probarse el modelo antes de haber completamente ajustado el mismo.\n",
    "\n",
    "Si se ajusta el modelo después de hacer la prueba, se empezará a sobre-ajustar el conjunto de datos de prueba, que echaría \"por los suelos\" el propósito original del mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 720us/step - accuracy: 0.9622 - loss: 19.0089\n"
     ]
    }
   ],
   "source": [
    "perdida_prueba, precision_prueba = modelo.evaluate(datos_prueba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pérdida de prueba: 19.01. Precisión de prueba: 96.22%\n"
     ]
    }
   ],
   "source": [
    "# Si se desea, se puede aplicar un formateo \"bonito\"\n",
    "print('Pérdida de prueba: {0:.2f}. Precisión de prueba: {1:.2f}%'.format(perdida_prueba, precision_prueba * 100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando el modelo inicial y los hiperparámetros dados en este notebook, la precisión de prueba final debe ser aproximadamente 97%.\n",
    "\n",
    "Cada vez que se ejecuta el código, se obtiene una precisión diferente debido a la \"barajeada\" de las tandas, los pesos se inicializan en forma diferente, etc.\n",
    "\n",
    "Finalmente, intencionalmente se ha llegado a una solución subóptima, para que puedan tener la oportunidad de mejorarla como ejercicio de laboratorio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nuevo Modelo\n",
    "En este modelo se hará los siguientes cambios:\n",
    "- Modificación de ancho de red\n",
    "- Modificación de profundidad \n",
    "- Experimentación con redes profundas\n",
    "- Funciones de activación\n",
    "- Modificación de la tanda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modificación del ancho de red\n",
    "Se modifica el tamaño de capa escondida a 200 y se vuelve a entrenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "tamanio_capa_escondida = 300 # modify hidden layer size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = tf.keras.Sequential([\n",
    "\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)), # capa entrada\n",
    "    \n",
    "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='relu'), # 1era capa escondida\n",
    "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='relu'), # 2nda capa escondida\n",
    "\n",
    "    tf.keras.layers.Dense(tamanio_salida, activation='softmax') # capa salida\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "500/500 - 2s - 3ms/step - accuracy: 0.9235 - loss: 0.2600 - val_accuracy: 0.9649 - val_loss: 0.1171\n",
      "Epoch 2/5\n",
      "500/500 - 1s - 2ms/step - accuracy: 0.9703 - loss: 0.0978 - val_accuracy: 0.9698 - val_loss: 0.1034\n",
      "Epoch 3/5\n",
      "500/500 - 1s - 2ms/step - accuracy: 0.9806 - loss: 0.0632 - val_accuracy: 0.9776 - val_loss: 0.0776\n",
      "Epoch 4/5\n",
      "500/500 - 1s - 2ms/step - accuracy: 0.9859 - loss: 0.0438 - val_accuracy: 0.9745 - val_loss: 0.0882\n",
      "Epoch 5/5\n",
      "500/500 - 1s - 2ms/step - accuracy: 0.9891 - loss: 0.0334 - val_accuracy: 0.9759 - val_loss: 0.0889\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x15ddb895f70>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUMERO_EPOCAS = 5\n",
    "\n",
    "modelo.fit(datos_entreno,\n",
    "          epochs = NUMERO_EPOCAS, \n",
    "          validation_data = datos_validacion,\n",
    "          verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9774 - loss: 13.5997  \n"
     ]
    }
   ],
   "source": [
    "perdida_prueba, precision_prueba = modelo.evaluate(datos_prueba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pérdida de prueba: 13.60. Precisión de prueba: 97.74%\n"
     ]
    }
   ],
   "source": [
    "print('Pérdida de prueba: {0:.2f}. Precisión de prueba: {1:.2f}%'.format(perdida_prueba, precision_prueba * 100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados\n",
    "1. ¿Cómo cambia la precisión de validación del modelo?<br>\n",
    "En este caso tuvimos menor pérdida con 200 que con el valor anterior de 50. La precisión fue un poco más elevada \n",
    "2. ¿Cuánto tiempo tarda el algoritmo en entrenar?<br>\n",
    "Con 200 capas escondidas tardó bastante poco. Unos cuantos milisegundos.\n",
    "3. ¿Cuál ofrece mejor rendimiento?<br>\n",
    "Con la de 200 tuvimos pérdida baja, similarmente con 100 hubo unos resultados similares. Ya cuando intentamos utilizar 300<br>\n",
    "la cantidad de pérdida fue aumentando aunque la precisión fue similar. Mientras que con 500 capas, la pérdida fue ligeramente menor pero aumentó por igual\n",
    "y con una precisión de casi 98%. <br>\n",
    "Por lo que podemos ver que realmente el rendimiento se mantiene bastante bueno, pero entre más capas, hay menor pérdida, pero más tiempo de entrenamiento.\n",
    "Por lo que vimos que en rendimiento y precisión era menor 300 capas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modificación de la profundiad de la red\n",
    "En este caso le agregaremos una nueva capa oculta al modelo.<br>\n",
    "De momento los valores a utilizar son:\n",
    "* tamaño de capa : 300\n",
    "* tamaño entrada : 784\n",
    "* tamaño salida : 10<br>\n",
    "Se utiliza<br>\n",
    "* activación ReLU para cada capa escondida\n",
    "* activación softmax para la de salida\n",
    "* optimizador adam\n",
    "* pérdida con entropía cruzada\n",
    "* 5 épocas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lpmon\\Documents\\GitHub\\Lab2-DataScience\\venv\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "modelo = tf.keras.Sequential([\n",
    "\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)), # capa entrada\n",
    "    \n",
    "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='relu'), # 1era capa escondida\n",
    "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='relu'), # 2nda capa escondida\n",
    "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='relu'), # 3ra capa escondida agregada\n",
    "\n",
    "    tf.keras.layers.Dense(tamanio_salida, activation='softmax') # capa salida\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "500/500 - 2s - 4ms/step - accuracy: 0.9236 - loss: 0.2533 - val_accuracy: 0.9616 - val_loss: 0.1275\n",
      "Epoch 2/5\n",
      "500/500 - 1s - 3ms/step - accuracy: 0.9714 - loss: 0.0938 - val_accuracy: 0.9722 - val_loss: 0.0936\n",
      "Epoch 3/5\n",
      "500/500 - 1s - 3ms/step - accuracy: 0.9803 - loss: 0.0626 - val_accuracy: 0.9732 - val_loss: 0.0924\n",
      "Epoch 4/5\n",
      "500/500 - 1s - 3ms/step - accuracy: 0.9842 - loss: 0.0467 - val_accuracy: 0.9761 - val_loss: 0.0830\n",
      "Epoch 5/5\n",
      "500/500 - 1s - 3ms/step - accuracy: 0.9879 - loss: 0.0375 - val_accuracy: 0.9781 - val_loss: 0.0760\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x15de328d310>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo.fit(datos_entreno,\n",
    "          epochs = NUMERO_EPOCAS, \n",
    "          validation_data = datos_validacion,\n",
    "          verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9794 - loss: 13.6910\n"
     ]
    }
   ],
   "source": [
    "perdida_prueba, precision_prueba = modelo.evaluate(datos_prueba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados\n",
    "1. Precisión de validación con el anterior modelo,<br>\n",
    "Este modelo presenta una validación similar a la del modelo anterior. Su exactitud es similar y su pérdida también. \n",
    "2. Impacto en tiempo de ejecución.<br>\n",
    "El tiempo de ejecución se vio levemente afectado por el añadimiento de una capa escondida extra.\n",
    "3. Cambios necesarios al código.<br>\n",
    "El código se mantuvi similar, solamente se tuvo que agregar la línea <br>\n",
    "`tf.keras.layers.Dense(tamanio_capa_escondida, activation='relu'),` <br>\n",
    "al modelo para que agregarse una capa escondida. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes Profundas\n",
    "Aquí experimentaremos con el ancho de cada capa y cómo afecta esto en su tiempo de ejecución. <br>\n",
    "Veremos si existe problemas de desvanecimiento de gradiente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = tf.keras.Sequential([\n",
    "\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)), # capa entrada\n",
    "    \n",
    "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='relu'), # 1era capa escondida\n",
    "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='relu'), # 2nda capa escondida\n",
    "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='relu'), # 3ra capa escondida agregada\n",
    "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='relu'), # 4ta capa escondida agregada \n",
    "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='relu'), # 5ta capa escondida agregada \n",
    "\n",
    "    tf.keras.layers.Dense(tamanio_salida, activation='softmax') # capa salida\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "500/500 - 2s - 5ms/step - accuracy: 0.9216 - loss: 0.2582 - val_accuracy: 0.9635 - val_loss: 0.1265\n",
      "Epoch 2/5\n",
      "500/500 - 2s - 3ms/step - accuracy: 0.9689 - loss: 0.1044 - val_accuracy: 0.9668 - val_loss: 0.1050\n",
      "Epoch 3/5\n",
      "500/500 - 2s - 3ms/step - accuracy: 0.9773 - loss: 0.0723 - val_accuracy: 0.9716 - val_loss: 0.0969\n",
      "Epoch 4/5\n",
      "500/500 - 2s - 3ms/step - accuracy: 0.9813 - loss: 0.0581 - val_accuracy: 0.9709 - val_loss: 0.1076\n",
      "Epoch 5/5\n",
      "500/500 - 2s - 3ms/step - accuracy: 0.9857 - loss: 0.0462 - val_accuracy: 0.9770 - val_loss: 0.0853\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x15e25f7a9c0>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo.fit(datos_entreno,\n",
    "          epochs = NUMERO_EPOCAS, \n",
    "          validation_data = datos_validacion,\n",
    "          verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9731 - loss: 17.0010 \n"
     ]
    }
   ],
   "source": [
    "perdida_prueba, precision_prueba = modelo.evaluate(datos_prueba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados\n",
    "1. Precisión de validación para cada intento<br>\n",
    "    - 4 capas: accuracy 97.54% / pérdida 15.35\n",
    "    - 5 capas: accuracy 97.31% / pérdida 17.00\n",
    "2. Relación entre profundidad y tiempo de ejecución<br>\n",
    "    Como podemos ver, entre más capas el tiempo de ejecución del modelo va demorando poco a poco<br>\n",
    "    cada vez más. Esto se debe sencillamente a que entre más capas tenemos, hay más transformaciones e interacciones <br>\n",
    "    entre las capas neuronales. Lo que causa que se demore cada vez más conforme se aumenta el número de capas.\n",
    "3. Problemas de desvanecimiento de gradiente<br>\n",
    "    Entre más capas neuronales tenemos, más activaciones estaremos haciendo, lo cual puede llevar a que\n",
    "    los coeficientes vayan siendo más y más pequeños conforme se agregan más capas debido a que si los <br>\n",
    "    coeficientes son menores a 1, la multiplicación entre estos dará como resultado un número más y más pequeño<br>\n",
    "    entre más capas pase. Por eso a veces más capas no significa mejor resultado. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones de Activación I\n",
    "Aplicaremos funciones de activación sigmoidales a todas las capas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = tf.keras.Sequential([\n",
    "\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)), # capa entrada\n",
    "    \n",
    "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='sigmoid'), # 1era capa escondida\n",
    "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='sigmoid'), # 2nda capa escondida\n",
    "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='sigmoid'), # 3ra capa escondida agregada\n",
    "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='sigmoid'), # 4ta capa escondida agregada \n",
    "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='sigmoid'), # 5ta capa escondida agregada \n",
    "\n",
    "    tf.keras.layers.Dense(tamanio_salida, activation='softmax') # capa salida\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "500/500 - 2s - 5ms/step - accuracy: 0.7038 - loss: 0.8700 - val_accuracy: 0.9127 - val_loss: 0.3124\n",
      "Epoch 2/5\n",
      "500/500 - 2s - 4ms/step - accuracy: 0.9236 - loss: 0.2703 - val_accuracy: 0.9462 - val_loss: 0.1890\n",
      "Epoch 3/5\n",
      "500/500 - 2s - 4ms/step - accuracy: 0.9482 - loss: 0.1802 - val_accuracy: 0.9551 - val_loss: 0.1554\n",
      "Epoch 4/5\n",
      "500/500 - 2s - 4ms/step - accuracy: 0.9599 - loss: 0.1399 - val_accuracy: 0.9608 - val_loss: 0.1349\n",
      "Epoch 5/5\n",
      "500/500 - 2s - 4ms/step - accuracy: 0.9671 - loss: 0.1134 - val_accuracy: 0.9608 - val_loss: 0.1389\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x15e261a8e90>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo.fit(datos_entreno,\n",
    "          epochs = NUMERO_EPOCAS, \n",
    "          validation_data = datos_validacion,\n",
    "          verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9536 - loss: 0.1758\n"
     ]
    }
   ],
   "source": [
    "perdida_prueba, precision_prueba = modelo.evaluate(datos_prueba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados\n",
    "1. Compare resultados anteriores\n",
    "    - 3 capas sigmoiodales : accuracy 96.26% / pérdida 0.12\n",
    "    - 3 capas ReLU : accuracy 97.94 / pérdida 13.69\n",
    "    - 4 capas sigmoidales : 95.61 / pérdida 0.1605\n",
    "    - 4 capas ReLU : accuracy 97.54% / pérdida 15.35\n",
    "    - 5 capas sigmoidales : 95.36 / pérdida 0.1758\n",
    "    - 5 capas ReLU : accuracy 97.31% / pérdida 17.00\n",
    "2. Analice el impacto de velocidad de convergencia<br>\n",
    "    La velocidad de convergencia es un poco diferente pero en este caso parece ser mejor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones de Activación II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = tf.keras.Sequential([\n",
    "\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)), # capa entrada\n",
    "    \n",
    "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='relu'), # 1era capa escondida\n",
    "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='tanh'), # 2nda capa escondida\n",
    "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='sigmoid'), # 3ra capa escondida agregada\n",
    "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='sigmoid'), # 4ta capa escondida agregada \n",
    "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='sigmoid'), # 5ta capa escondida agregada \n",
    "\n",
    "    tf.keras.layers.Dense(tamanio_salida, activation='softmax') # capa salida\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "500/500 - 2s - 5ms/step - accuracy: 0.8627 - loss: 0.4503 - val_accuracy: 0.9573 - val_loss: 0.1543\n",
      "Epoch 2/5\n",
      "500/500 - 2s - 3ms/step - accuracy: 0.9632 - loss: 0.1277 - val_accuracy: 0.9681 - val_loss: 0.1128\n",
      "Epoch 3/5\n",
      "500/500 - 2s - 3ms/step - accuracy: 0.9752 - loss: 0.0860 - val_accuracy: 0.9690 - val_loss: 0.1152\n",
      "Epoch 4/5\n",
      "500/500 - 2s - 4ms/step - accuracy: 0.9808 - loss: 0.0649 - val_accuracy: 0.9710 - val_loss: 0.1080\n",
      "Epoch 5/5\n",
      "500/500 - 2s - 4ms/step - accuracy: 0.9856 - loss: 0.0486 - val_accuracy: 0.9733 - val_loss: 0.1034\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x15e35aaba40>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo.fit(datos_entreno,\n",
    "          epochs = NUMERO_EPOCAS, \n",
    "          validation_data = datos_validacion,\n",
    "          verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9736 - loss: 0.1035\n"
     ]
    }
   ],
   "source": [
    "perdida_prueba, precision_prueba = modelo.evaluate(datos_prueba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados\n",
    "1. Comparación entre modelos.\n",
    "    - 3 capas sigmoiodales : accuracy 96.26% / pérdida 0.12\n",
    "    - 3 capas ReLU : accuracy 97.47% / pérdida 0.0995\n",
    "    - 3 capas ReLU & Tanh : accuracy 97.45% / pérdida 0.1220\n",
    "    - 4 capas sigmoidales : 95.61% / pérdida 0.1605\n",
    "    - 4 capas ReLU : accuracy 97.54% / pérdida 15.35\n",
    "    - 4 capas ReLU & Tanh : accuracy 97.40% / pérdida 0.1023\n",
    "    - 5 capas sigmoidales : 95.36% / pérdida 0.1758\n",
    "    - 5 capas ReLU : accuracy 97.31% / pérdida 17.00\n",
    "    - 5 capas ReLU & Tanh : accuracy 97.36% / pérdida 0.1035\n",
    "2. Las ventajas y desventajas.<br>\n",
    "    ReLU: <br>\n",
    "    - ventaja:\n",
    "    - desventaja: \n",
    "    Sigmoidal: <br>\n",
    "    - ventaja: \n",
    "    - desventaja: \n",
    "    Tanh: <br>\n",
    "    - ventaja:\n",
    "    - desventaja: \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modificar el tamaño del batch\n",
    "En este caso vamos a modificar el tamaño del batch a 10000 para ver los cambios de desempeño y pérdida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAMANIO_TANDA = 10000\n",
    "datos_entreno = datos_entreno.shuffle(buffer_size = num_obs_entreno).batch(TAMANIO_TANDA)\n",
    "datos_validacion = datos_validacion.batch(TAMANIO_TANDA)\n",
    "datos_prueba = datos_prueba.batch(TAMANIO_TANDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lpmon\\Documents\\GitHub\\Lab2-DataScience\\venv\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "modelo = tf.keras.Sequential([\n",
    "\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)), # capa entrada\n",
    "    \n",
    "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='relu'), # 1era capa escondida\n",
    "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='tanh'), # 2nda capa escondida\n",
    "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='sigmoid'), # 3ra capa escondida agregada\n",
    "    # tf.keras.layers.Dense(tamanio_capa_escondida, activation='sigmoid'), # 4ta capa escondida agregada \n",
    "    # tf.keras.layers.Dense(tamanio_capa_escondida, activation='sigmoid'), # 5ta capa escondida agregada \n",
    "\n",
    "    tf.keras.layers.Dense(tamanio_salida, activation='softmax') # capa salida\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"Cast:0\", shape=(None, None, 28, 28), dtype=float32). Expected shape (None, 28, 28), but input has incompatible shape (None, None, 28, 28)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(None, None, 28, 28), dtype=float32)\n  • training=True\n  • mask=None\n  • kwargs=<class 'inspect._empty'>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[128]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodelo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatos_entreno\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m          \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUMERO_EPOCAS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m          \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatos_validacion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m          \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lpmon\\Documents\\GitHub\\Lab2-DataScience\\venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lpmon\\Documents\\GitHub\\Lab2-DataScience\\venv\\Lib\\site-packages\\keras\\src\\models\\functional.py:276\u001b[39m, in \u001b[36mFunctional._adjust_input_rank\u001b[39m\u001b[34m(self, flat_inputs)\u001b[39m\n\u001b[32m    274\u001b[39m             adjusted.append(ops.expand_dims(x, axis=-\u001b[32m1\u001b[39m))\n\u001b[32m    275\u001b[39m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m276\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    277\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid input shape for input \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Expected shape \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    278\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mref_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, but input has incompatible shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    279\u001b[39m     )\n\u001b[32m    280\u001b[39m \u001b[38;5;66;03m# Add back metadata.\u001b[39;00m\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(flat_inputs)):\n",
      "\u001b[31mValueError\u001b[39m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"Cast:0\", shape=(None, None, 28, 28), dtype=float32). Expected shape (None, 28, 28), but input has incompatible shape (None, None, 28, 28)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(None, None, 28, 28), dtype=float32)\n  • training=True\n  • mask=None\n  • kwargs=<class 'inspect._empty'>"
     ]
    }
   ],
   "source": [
    "modelo.fit(datos_entreno,\n",
    "          epochs = NUMERO_EPOCAS, \n",
    "          validation_data = datos_validacion,\n",
    "          verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perdida_prueba, precision_prueba = modelo.evaluate(datos_prueba)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
